{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Size for Classification of Citrus Leaves\n",
    "See `citrus_leaves.ipynb` for main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.io import read_image # use PyTorch to read images\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"./data_citrus_leaves/\"\n",
    "TRAIN_PROPORTION = 0.8\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 40\n",
    "IMAGE_SIZE = (256, 256)\n",
    "LR = 0.001\n",
    "RESNET_MEAN = [0.485, 0.456, 0.406]\n",
    "RESNET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple encoder to convert `string` to `int`\n",
    "encoder = {\n",
    "    \"healthy\" : 0,\n",
    "    \"black_spot\" : 1,\n",
    "    \"canker\" : 2,\n",
    "    \"greening\" : 3,\n",
    "    \"healthy\" : 4,\n",
    "    \"melanose\" : 5\n",
    "}\n",
    "\n",
    "decoder = {\n",
    "    0 : \"healthy\",\n",
    "    1 : \"black_spot\",\n",
    "    2 : \"canker\",\n",
    "    3 : \"greening\",\n",
    "    4 : \"healthy\",\n",
    "    5 : \"melanose\"\n",
    "}\n",
    "\n",
    "# class label is name of sub directory, images are in sub directory\n",
    "class CitrusLeavesDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform = None, target_transform = None, length = None):\n",
    "        # define main directory of images and transformations\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # store paths and labels in DataFrame\n",
    "        data = { \"image_path\" : [], \"label\" : [] }\n",
    "        for sub_dir in os.listdir(img_dir):\n",
    "            for file in os.listdir(os.path.join(img_dir, sub_dir)):\n",
    "                data['image_path'].append(os.path.join(img_dir, sub_dir, file))\n",
    "                data['label'].append(sub_dir)\n",
    "\n",
    "        # store in annotations\n",
    "        self.annotations = pd.DataFrame(data).sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "        # if length is specified, only use that many samples\n",
    "        if length is not None:\n",
    "            if length > len(self.annotations) or length < 1:\n",
    "                raise ValueError(\"Length must be between 1 and \" + str(len(self.annotations)))\n",
    "            self.annotations = self.annotations[:length]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get image path and label\n",
    "        image = read_image(self.annotations.iloc[idx, 0]).float()\n",
    "        label = encoder[self.annotations.iloc[idx, 1]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(device = None, learning_rate = LR):\n",
    "    # load model\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained = True)\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "    # create model\n",
    "    model = nn.Sequential(\n",
    "        feature_extractor,\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(2048, 6)\n",
    "    )\n",
    "    \n",
    "    if device is not None:\n",
    "        model = model.to(device)\n",
    "\n",
    "    # define loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "def train_model(model, loader, optimizer, criterion, device = None, epochs = NUM_EPOCHS, stats = False):\n",
    "    # train model\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, labels) in enumerate(loader):\n",
    "            if device is not None:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if stats:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"[{}, {}] Loss: {}\".format(epoch, i, loss.item()))\n",
    "\n",
    "    return losses\n",
    "\n",
    "def test_model(model, loader, device = None):\n",
    "    # test model\n",
    "    model.eval()\n",
    "    predict = torch.tensor([], dtype = torch.long)\n",
    "    actual = torch.tensor([], dtype = torch.long)\n",
    "\n",
    "    if device is not None:\n",
    "        predict = predict.to(device)\n",
    "        actual = actual.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            if device is not None:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predict = torch.cat((predict, predicted))\n",
    "            actual = torch.cat((actual, labels))\n",
    "\n",
    "    return predict, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\fongc/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n",
      "c:\\Users\\fongc\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\fongc\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 100: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\fongc/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n",
      "c:\\Users\\fongc\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\fongc\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 200: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\fongc/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n",
      "c:\\Users\\fongc\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\fongc\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 300: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\fongc/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n",
      "c:\\Users\\fongc\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\fongc\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# iterate through sizes\n",
    "d = {}\n",
    "for length in [ 100, 200, 300, 400, 500, 600 ]:\n",
    "    d[length] = {}\n",
    "\n",
    "    # load data\n",
    "    data = CitrusLeavesDataset(\n",
    "        img_dir = IMG_DIR,\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.Normalize(mean = RESNET_MEAN, std = RESNET_STD)\n",
    "        ]),\n",
    "        length = length\n",
    "    )\n",
    "\n",
    "    train_size = int(TRAIN_PROPORTION * len(data))\n",
    "    train, test = random_split(data, [ train_size, len(data) - train_size ])\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    test_loader = DataLoader(test, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "    # create model\n",
    "    model, criterion, optimizer = create_model(device = device)\n",
    "\n",
    "    # train model\n",
    "    losses = train_model(model, train_loader, optimizer, criterion, device = device)\n",
    "    d[length]['losses'] = losses\n",
    "\n",
    "    # test model\n",
    "    predict, actual = test_model(model, test_loader, device = device)\n",
    "    d[length]['predict'] = predict\n",
    "    d[length]['actual'] = actual\n",
    "\n",
    "    print(\"Length {}: {}\".format(\n",
    "        length,\n",
    "        metrics.accuracy_score(actual.cpu().numpy(), predict.cpu().numpy())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b13c84f26b03b81837babb7d6bb182b97bb46d822613cfd30489287e68eae110"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
